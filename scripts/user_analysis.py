#!/usr/bin/env python3
"""
é–‹ç™¼è€…æŠ€è¡“æ°´å¹³åˆ†ææ¨¡çµ„

æä¾›å…©ç¨®åˆ†ææ–¹å¼ï¼š
1. CodeBasedAnalyzer: åŸºæ–¼ç¨‹å¼ç¢¼è¨ˆç®—çš„è©•åˆ†ç³»çµ±
2. AIModelAnalyzer: åŸºæ–¼ GitHub Models API çš„ AI åˆ†æ
"""

import os
import re
import json
import requests
from abc import ABC, abstractmethod
from typing import Optional, Dict, Any, List
from pathlib import Path
import pandas as pd
from datetime import datetime
from collections import Counter, defaultdict

import config
from progress_reporter import IProgressReporter, SilentProgressReporter


# ==================== æŠ½è±¡ä»‹é¢ ====================

class IUserAnalyzer(ABC):
    """é–‹ç™¼è€…åˆ†æä»‹é¢"""
    
    @abstractmethod
    def analyze(self, user_data_dir: Path, spec_file: Optional[Path] = None) -> str:
        """
        åˆ†æé–‹ç™¼è€…æŠ€è¡“æ°´å¹³
        
        Args:
            user_data_dir: ä½¿ç”¨è€…è³‡æ–™ç›®éŒ„ï¼ˆåŒ…å« CSV æª”æ¡ˆï¼‰
            spec_file: åˆ†æè¦æ ¼æª”æ¡ˆè·¯å¾‘
            
        Returns:
            åˆ†æå ±å‘Šï¼ˆMarkdown æ ¼å¼ï¼‰
        """
        pass


# ==================== è³‡æ–™è®€å–å™¨ ====================

class UserDataLoader:
    """ä½¿ç”¨è€…è³‡æ–™è¼‰å…¥å™¨"""
    
    def __init__(self, user_data_dir: Path):
        self.user_data_dir = user_data_dir
        self.data: Dict[str, pd.DataFrame] = {}
    
    def load_all(self) -> Dict[str, pd.DataFrame]:
        """è¼‰å…¥æ‰€æœ‰ CSV æª”æ¡ˆ"""
        csv_files = {
            'commits': 'commits.csv',
            'statistics': 'statistics.csv',
            'code_reviews': 'code_reviews.csv',
            'code_changes': 'code_changes.csv',
            'user_events': 'user_events.csv',
            'merge_requests': 'merge_requests.csv',
            'user_profile': 'user_profile.csv'
        }
        
        for key, filename in csv_files.items():
            file_path = self.user_data_dir / filename
            if file_path.exists():
                try:
                    # ä½¿ç”¨ utf-8-sig è™•ç† BOM
                    df = pd.read_csv(file_path, encoding='utf-8-sig')
                    self.data[key] = df
                except Exception as e:
                    print(f"âš ï¸ è­¦å‘Šï¼šç„¡æ³•è®€å– {filename}: {e}")
                    self.data[key] = pd.DataFrame()
            else:
                self.data[key] = pd.DataFrame()
        
        return self.data
    
    def get_username(self) -> str:
        """å¾ç›®éŒ„åç¨±æˆ– user_profile å–å¾—ä½¿ç”¨è€…åç¨±"""
        # å„ªå…ˆå¾ user_profile å–å¾—
        if not self.data.get('user_profile', pd.DataFrame()).empty:
            profile = self.data['user_profile'].iloc[0]
            return profile.get('username', self.user_data_dir.name)
        
        # å¦å‰‡ä½¿ç”¨ç›®éŒ„åç¨±
        return self.user_data_dir.name


# ==================== æ–¹æ¡ˆ B: ç¨‹å¼ç¢¼è¨ˆç®—åˆ†æå™¨ ====================

class CodeBasedAnalyzer(IUserAnalyzer):
    """åŸºæ–¼ç¨‹å¼ç¢¼è¨ˆç®—çš„è©•åˆ†ç³»çµ±"""
    
    def __init__(self, progress_reporter: Optional[IProgressReporter] = None):
        self.progress = progress_reporter or SilentProgressReporter()
        self.data_loader: Optional[UserDataLoader] = None
        self.data: Dict[str, pd.DataFrame] = {}
        self.scores: Dict[str, float] = {}
        self.total_score: float = 0.0
        self.level: str = ""
    
    def analyze(self, user_data_dir: Path, spec_file: Optional[Path] = None) -> str:
        """åŸ·è¡Œåˆ†æ"""
        self.progress.report_start(f"æ­£åœ¨åˆ†æ {user_data_dir.name}...")
        
        # è¼‰å…¥è³‡æ–™
        self.data_loader = UserDataLoader(user_data_dir)
        self.data = self.data_loader.load_all()
        
        if self.data.get('commits', pd.DataFrame()).empty:
            return f"# {user_data_dir.name} æŠ€è¡“æ°´å¹³åˆ†æå ±å‘Š\n\nâš ï¸ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° commits.csv æˆ–è³‡æ–™ç‚ºç©º"
        
        # è¨ˆç®—å„ç¶­åº¦è©•åˆ†
        self.scores = {
            'contribution': self._calculate_contribution_score(),
            'commit_quality': self._calculate_commit_quality_score(),
            'tech_breadth': self._calculate_tech_breadth_score(),
            'collaboration': self._calculate_collaboration_score(),
            'code_review': self._calculate_code_review_score(),
            'work_pattern': self._calculate_work_pattern_score(),
            'progress_trend': self._calculate_progress_trend_score()
        }
        
        # è¨ˆç®—ç¸½åˆ†
        total_score = self._calculate_total_score()
        level = self._determine_level(total_score)
        
        # å„²å­˜ç¸½åˆ†å’Œç­‰ç´šï¼ˆä¾›å½™ç¸½å ±å‘Šä½¿ç”¨ï¼‰
        self.total_score = total_score
        self.level = level
        
        # ç”¢ç”Ÿå ±å‘Š
        report = self._generate_markdown_report(total_score, level)
        
        self.progress.report_complete(f"åˆ†æå®Œæˆï¼š{level}ï¼ˆ{total_score:.2f}/10ï¼‰")
        
        return report
    
    # ========== ç¶­åº¦ 1: ç¨‹å¼ç¢¼è²¢ç»é‡ (12%) ==========
    
    def _calculate_contribution_score(self) -> float:
        """è¨ˆç®—ç¨‹å¼ç¢¼è²¢ç»é‡è©•åˆ†"""
        if self.data['statistics'].empty:
            return 5.0
        
        stats = self.data['statistics'].iloc[0]
        total_commits = int(stats.get('total_commits', 0))
        
        # æ ¹æ“šæäº¤æ¬¡æ•¸è©•åˆ†
        if total_commits >= 200:
            return 10.0
        elif total_commits >= 100:
            return 8.0
        elif total_commits >= 50:
            return 6.0
        else:
            return 4.0
    
    # ========== ç¶­åº¦ 2: Commit å“è³ª (23%) ==========
    
    def _calculate_commit_quality_score(self) -> float:
        """è¨ˆç®— Commit å“è³ªè©•åˆ†"""
        if self.data['commits'].empty:
            return 5.0
        
        commits_df = self.data['commits']
        
        # A. Message è¦ç¯„æ€§ (40%)
        message_score = self._calculate_message_quality(commits_df)
        
        # B. è®Šæ›´ç²’åº¦ (40%)
        granularity_score = self._calculate_change_granularity(commits_df)
        
        # C. ä¿®å¾©æ€§æäº¤æ¯”ä¾‹ (20%)
        fix_ratio_score = self._calculate_fix_ratio(commits_df)
        
        # åŠ æ¬Šå¹³å‡
        quality_score = (message_score * 0.4 + 
                        granularity_score * 0.4 + 
                        fix_ratio_score * 0.2)
        
        return quality_score
    
    def _calculate_message_quality(self, commits_df: pd.DataFrame) -> float:
        """è¨ˆç®— Commit Message å“è³ª"""
        # Conventional Commits è¦ç¯„
        conventional_pattern = r'^(feat|fix|docs|refactor|test|chore|style|perf)(\(.+\))?:'
        
        total = len(commits_df)
        if total == 0:
            return 5.0
        
        # è¨ˆç®—ç¬¦åˆè¦ç¯„çš„æ¯”ä¾‹
        conventional_count = commits_df['title'].str.contains(
            conventional_pattern, 
            case=False, 
            regex=True,
            na=False
        ).sum()
        
        conventional_ratio = conventional_count / total
        
        # è©•åˆ†
        if conventional_ratio >= 0.8:
            return 10.0
        elif conventional_ratio >= 0.6:
            return 8.0
        elif conventional_ratio >= 0.4:
            return 6.0
        else:
            return 4.0
    
    def _calculate_change_granularity(self, commits_df: pd.DataFrame) -> float:
        """è¨ˆç®—è®Šæ›´ç²’åº¦è©•åˆ†"""
        # å°å‹è®Šæ›´ï¼šâ‰¤100 è¡Œ
        # ä¸­å‹è®Šæ›´ï¼š100-500 è¡Œ
        # å¤§å‹è®Šæ›´ï¼š>500 è¡Œ
        
        total = len(commits_df)
        if total == 0:
            return 5.0
        
        commits_df['total_changes'] = commits_df['additions'] + commits_df['deletions']
        
        small_count = (commits_df['total_changes'] <= 100).sum()
        medium_count = ((commits_df['total_changes'] > 100) & 
                       (commits_df['total_changes'] <= 500)).sum()
        large_count = (commits_df['total_changes'] > 500).sum()
        
        small_ratio = small_count / total
        
        # è©•åˆ†ï¼šå°å‹è®Šæ›´ä½”æ¯”è¶Šé«˜è¶Šå¥½
        if small_ratio >= 0.6:
            return 10.0
        elif small_ratio >= 0.4:
            return 7.0
        else:
            return 5.0
    
    def _calculate_fix_ratio(self, commits_df: pd.DataFrame) -> float:
        """è¨ˆç®—ä¿®å¾©æ€§æäº¤æ¯”ä¾‹"""
        total = len(commits_df)
        if total == 0:
            return 5.0
        
        # çµ±è¨ˆåŒ…å«ä¿®å¾©é—œéµå­—çš„æäº¤
        fix_pattern = r'(fix|bug|hotfix|revert)'
        fix_count = commits_df['title'].str.contains(
            fix_pattern,
            case=False,
            regex=True,
            na=False
        ).sum()
        
        fix_ratio = fix_count / total
        
        # è©•åˆ†ï¼šä¿®å¾©ç‡è¶Šä½è¶Šå¥½
        if fix_ratio < 0.15:
            return 10.0
        elif fix_ratio < 0.30:
            return 7.0
        else:
            return 4.0
    
    # ========== ç¶­åº¦ 3: æŠ€è¡“å»£åº¦ (18%) ==========
    
    def _calculate_tech_breadth_score(self) -> float:
        """è¨ˆç®—æŠ€è¡“å»£åº¦è©•åˆ†"""
        if self.data['code_changes'].empty:
            return 5.0
        
        changes_df = self.data['code_changes']
        
        # æå–æª”æ¡ˆå‰¯æª”å
        file_extensions = changes_df['file_path'].apply(
            lambda x: os.path.splitext(str(x))[1].lower() if pd.notna(x) else ''
        )
        
        # éæ¿¾æ‰ç©ºå­—ä¸²å’Œå¸¸è¦‹éç¨‹å¼ç¢¼æª”æ¡ˆ
        ignore_extensions = {'', '.md', '.txt', '.json', '.yml', '.yaml', '.xml'}
        file_extensions = file_extensions[~file_extensions.isin(ignore_extensions)]
        
        # çµ±è¨ˆä¸åŒå‰¯æª”åæ•¸é‡
        unique_extensions = file_extensions.nunique()
        
        # è©•åˆ†
        if unique_extensions >= 5:
            return 10.0
        elif unique_extensions >= 3:
            return 8.0
        elif unique_extensions >= 1:
            return 6.0
        else:
            return 4.0
    
    # ========== ç¶­åº¦ 4: å”ä½œèƒ½åŠ› (12%) ==========
    
    def _calculate_collaboration_score(self) -> float:
        """è¨ˆç®—å”ä½œèƒ½åŠ›è©•åˆ†"""
        if self.data['commits'].empty:
            return 5.0
        
        commits_df = self.data['commits']
        
        # A. Merge Commits åƒèˆ‡åº¦
        merge_pattern = r'merge'
        merge_count = commits_df['title'].str.contains(
            merge_pattern,
            case=False,
            regex=True,
            na=False
        ).sum()
        
        total_commits = len(commits_df)
        merge_ratio = merge_count / total_commits if total_commits > 0 else 0
        
        # B. Revert ç‡ï¼ˆè¶Šä½è¶Šå¥½ï¼‰
        revert_pattern = r'revert'
        revert_count = commits_df['title'].str.contains(
            revert_pattern,
            case=False,
            regex=True,
            na=False
        ).sum()
        
        revert_ratio = revert_count / total_commits if total_commits > 0 else 0
        
        # è©•åˆ†
        score = 7.0  # åŸºç¤åˆ†
        
        # Merge åƒèˆ‡åº¦åŠ åˆ†
        if merge_ratio > 0.1:
            score += 2.0
        elif merge_ratio > 0.05:
            score += 1.0
        
        # Revert ç‡æ‰£åˆ†
        if revert_ratio > 0.05:
            score -= 3.0
        elif revert_ratio > 0.02:
            score -= 1.0
        
        return max(1.0, min(10.0, score))
    
    # ========== ç¶­åº¦ 5: Code Review å“è³ª (10%) ==========
    
    def _calculate_code_review_score(self) -> float:
        """è¨ˆç®— Code Review å“è³ªè©•åˆ†"""
        if self.data['code_reviews'].empty:
            return 5.0
        
        reviews_df = self.data['code_reviews']
        total_reviews = len(reviews_df)
        
        # ç°¡å–®è©•åˆ†ï¼šåŸºæ–¼åƒèˆ‡åº¦
        if total_reviews >= 20:
            return 9.0
        elif total_reviews >= 10:
            return 7.0
        elif total_reviews >= 5:
            return 6.0
        else:
            return 5.0
    
    # ========== ç¶­åº¦ 6: å·¥ä½œæ¨¡å¼ (10%) ==========
    
    def _calculate_work_pattern_score(self) -> float:
        """è¨ˆç®—å·¥ä½œæ¨¡å¼è©•åˆ†"""
        if self.data['user_events'].empty:
            return 5.0
        
        events_df = self.data['user_events']
        
        # å˜—è©¦è§£ææ™‚é–“
        try:
            events_df['created_at'] = pd.to_datetime(events_df['created_at'])
            events_df['hour'] = events_df['created_at'].dt.hour
            events_df['weekday'] = events_df['created_at'].dt.weekday
            
            # å·¥ä½œæ™‚æ®µ (9-18é») æ´»å‹•æ¯”ä¾‹
            work_hours = events_df['hour'].between(9, 18).sum()
            total_events = len(events_df)
            work_hours_ratio = work_hours / total_events if total_events > 0 else 0
            
            # å·¥ä½œæ—¥ï¼ˆé€±ä¸€åˆ°é€±äº”ï¼‰æ´»å‹•æ¯”ä¾‹
            work_days = events_df['weekday'].between(0, 4).sum()
            work_days_ratio = work_days / total_events if total_events > 0 else 0
            
            # è©•åˆ†
            score = 5.0
            if work_hours_ratio >= 0.6:
                score += 2.5
            if work_days_ratio >= 0.7:
                score += 2.5
            
            return min(10.0, score)
        except:
            return 5.0
    
    # ========== ç¶­åº¦ 7: é€²æ­¥è¶¨å‹¢ (15%) ==========
    
    def _calculate_progress_trend_score(self) -> float:
        """è¨ˆç®—é€²æ­¥è¶¨å‹¢è©•åˆ†"""
        if self.data['commits'].empty:
            return 5.0
        
        commits_df = self.data['commits']
        
        try:
            # è§£ææäº¤æ—¥æœŸ
            commits_df['committed_date'] = pd.to_datetime(commits_df['committed_date'])
            commits_df = commits_df.sort_values('committed_date')
            
            # è¨ˆç®—ä¸­ä½æ•¸æ—¥æœŸï¼Œåˆ†ç‚ºå‰å¾Œå…©æœŸ
            median_date = commits_df['committed_date'].median()
            
            early_commits = commits_df[commits_df['committed_date'] <= median_date]
            recent_commits = commits_df[commits_df['committed_date'] > median_date]
            
            if len(early_commits) == 0 or len(recent_commits) == 0:
                return 7.0  # è³‡æ–™ä¸è¶³ï¼Œçµ¦äºˆä¸­ç­‰åˆ†æ•¸
            
            # æ¯”è¼ƒå‰å¾ŒæœŸçš„ Commit Message å“è³ª
            early_quality = self._calculate_message_quality(early_commits)
            recent_quality = self._calculate_message_quality(recent_commits)
            
            # é€²æ­¥å¹…åº¦
            improvement = recent_quality - early_quality
            
            # è©•åˆ†
            if improvement >= 2.0:
                return 10.0
            elif improvement >= 1.0:
                return 8.5
            elif improvement >= 0:
                return 7.0
            else:
                return 5.0
        except:
            return 7.0
    
    # ========== ç¸½åˆ†è¨ˆç®— ==========
    
    def _calculate_total_score(self) -> float:
        """è¨ˆç®—ç¸½åˆ†ï¼ˆåŠ æ¬Šå¹³å‡ï¼‰"""
        weights = {
            'contribution': 0.12,
            'commit_quality': 0.23,
            'tech_breadth': 0.18,
            'collaboration': 0.12,
            'code_review': 0.10,
            'work_pattern': 0.10,
            'progress_trend': 0.15
        }
        
        total = sum(self.scores[key] * weights[key] for key in weights.keys())
        return round(total, 2)
    
    def _determine_level(self, total_score: float) -> str:
        """åˆ¤å®šç­‰ç´š"""
        if total_score >= 8.0:
            return "ğŸ† é«˜ç´šå·¥ç¨‹å¸«"
        elif total_score >= 5.0:
            return "â­ ä¸­ç´šå·¥ç¨‹å¸«"
        else:
            return "ğŸŒ± åˆç´šå·¥ç¨‹å¸«"
    
    # ========== å ±å‘Šç”¢ç”Ÿ ==========
    
    def _generate_markdown_report(self, total_score: float, level: str) -> str:
        """ç”¢ç”Ÿ Markdown æ ¼å¼å ±å‘Š"""
        username = self.data_loader.get_username() if self.data_loader else "Unknown"
        
        # å–å¾—åŸºæœ¬çµ±è¨ˆ
        stats = self.data['statistics'].iloc[0] if not self.data['statistics'].empty else {}
        total_commits = int(stats.get('total_commits', 0))
        total_additions = int(stats.get('total_additions', 0))
        total_deletions = int(stats.get('total_deletions', 0))
        
        report = f"""# {username} æŠ€è¡“æ°´å¹³åˆ†æå ±å‘Š

**ç”Ÿæˆæ™‚é–“ï¼š** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**åˆ†ææ–¹å¼ï¼š** ç¨‹å¼ç¢¼è¨ˆç®—ï¼ˆCode-Based Analysisï¼‰

---

## ğŸ“Š ç¸½é«”è©•ä¼°

| é …ç›® | æ•¸å€¼ |
|------|------|
| **ç¸½åˆ†** | **{total_score:.2f} / 10** |
| **ç­‰ç´š** | **{level}** |
| ç¸½æäº¤æ•¸ | {total_commits} |
| ç¸½æ–°å¢è¡Œæ•¸ | {total_additions:,} |
| ç¸½åˆªé™¤è¡Œæ•¸ | {total_deletions:,} |

---

## ğŸ¯ å„ç¶­åº¦è©•åˆ†

| ç¶­åº¦ | åˆ†æ•¸ | æ¬Šé‡ | åŠ æ¬Šåˆ†æ•¸ |
|------|------|------|----------|
| ç¨‹å¼ç¢¼è²¢ç»é‡ | {self.scores['contribution']:.2f} / 10 | 12% | {self.scores['contribution'] * 0.12:.2f} |
| **Commit å“è³ª** | **{self.scores['commit_quality']:.2f} / 10** | **23%** | **{self.scores['commit_quality'] * 0.23:.2f}** |
| æŠ€è¡“å»£åº¦ | {self.scores['tech_breadth']:.2f} / 10 | 18% | {self.scores['tech_breadth'] * 0.18:.2f} |
| å”ä½œèƒ½åŠ› | {self.scores['collaboration']:.2f} / 10 | 12% | {self.scores['collaboration'] * 0.12:.2f} |
| Code Review å“è³ª | {self.scores['code_review']:.2f} / 10 | 10% | {self.scores['code_review'] * 0.10:.2f} |
| å·¥ä½œæ¨¡å¼ | {self.scores['work_pattern']:.2f} / 10 | 10% | {self.scores['work_pattern'] * 0.10:.2f} |
| é€²æ­¥è¶¨å‹¢ | {self.scores['progress_trend']:.2f} / 10 | 15% | {self.scores['progress_trend'] * 0.15:.2f} |

---

## ğŸ“ è©³ç´°åˆ†æ

### 1ï¸âƒ£ ç¨‹å¼ç¢¼è²¢ç»é‡ ({self.scores['contribution']:.2f}/10)

{self._generate_contribution_details()}

### 2ï¸âƒ£ Commit å“è³ª ({self.scores['commit_quality']:.2f}/10) â­ æœ€é‡è¦

{self._generate_commit_quality_details()}

### 3ï¸âƒ£ æŠ€è¡“å»£åº¦ ({self.scores['tech_breadth']:.2f}/10)

{self._generate_tech_breadth_details()}

### 4ï¸âƒ£ å”ä½œèƒ½åŠ› ({self.scores['collaboration']:.2f}/10)

{self._generate_collaboration_details()}

### 5ï¸âƒ£ Code Review å“è³ª ({self.scores['code_review']:.2f}/10)

{self._generate_code_review_details()}

### 6ï¸âƒ£ å·¥ä½œæ¨¡å¼ ({self.scores['work_pattern']:.2f}/10)

{self._generate_work_pattern_details()}

### 7ï¸âƒ£ é€²æ­¥è¶¨å‹¢ ({self.scores['progress_trend']:.2f}/10)

{self._generate_progress_trend_details()}

---

## ğŸ’¡ æ”¹é€²å»ºè­°

{self._generate_improvement_suggestions(total_score)}

---

**åˆ†æå·¥å…·ç‰ˆæœ¬ï¼š** v1.0  
**è©•åˆ†æ¨™æº–ï¼š** åŸºæ–¼ code-quality-analysis-spec.md
"""
        
        return report
    
    def _generate_contribution_details(self) -> str:
        """ç”¢ç”Ÿè²¢ç»é‡è©³ç´°èªªæ˜"""
        if self.data['statistics'].empty:
            return "âš ï¸ ç„¡çµ±è¨ˆè³‡æ–™"
        
        stats = self.data['statistics'].iloc[0]
        total_commits = int(stats.get('total_commits', 0))
        
        if total_commits >= 200:
            level = "âœ… é«˜æ´»èºåº¦"
        elif total_commits >= 100:
            level = "â­ ç©©å®šè²¢ç»"
        elif total_commits >= 50:
            level = "ğŸ“š ä¸­ç­‰åƒèˆ‡"
        else:
            level = "ğŸŒ± åƒèˆ‡åº¦ä½"
        
        return f"""- ç¸½æäº¤æ•¸ï¼š**{total_commits}** ({level})
- è©•ä¼°ï¼š{'æ´»èºé–‹ç™¼è€…ï¼ŒæŒçºŒç©©å®šè²¢ç»' if total_commits >= 100 else 'å»ºè­°å¢åŠ ç¨‹å¼ç¢¼è²¢ç»é »ç‡'}"""
    
    def _generate_commit_quality_details(self) -> str:
        """ç”¢ç”Ÿ Commit å“è³ªè©³ç´°èªªæ˜"""
        if self.data['commits'].empty:
            return "âš ï¸ ç„¡ Commit è³‡æ–™"
        
        commits_df = self.data['commits']
        
        # Message è¦ç¯„æ€§
        conventional_pattern = r'^(feat|fix|docs|refactor|test|chore|style|perf)(\(.+\))?:'
        conventional_count = commits_df['title'].str.contains(
            conventional_pattern, case=False, regex=True, na=False
        ).sum()
        conventional_ratio = conventional_count / len(commits_df)
        
        # è®Šæ›´ç²’åº¦
        commits_df['total_changes'] = commits_df['additions'] + commits_df['deletions']
        small_count = (commits_df['total_changes'] <= 100).sum()
        small_ratio = small_count / len(commits_df)
        
        # ä¿®å¾©ç‡
        fix_count = commits_df['title'].str.contains(
            r'(fix|bug|hotfix|revert)', case=False, regex=True, na=False
        ).sum()
        fix_ratio = fix_count / len(commits_df)
        
        return f"""#### A. Message è¦ç¯„æ€§
- ç¬¦åˆ Conventional Commitsï¼š**{conventional_ratio*100:.1f}%** ({conventional_count}/{len(commits_df)})
- è©•ä¼°ï¼š{'âœ… å„ªç§€' if conventional_ratio >= 0.8 else 'âš ï¸ éœ€æ”¹é€²'}

#### B. è®Šæ›´ç²’åº¦
- å°å‹è®Šæ›´ï¼ˆâ‰¤100è¡Œï¼‰ï¼š**{small_ratio*100:.1f}%** ({small_count}/{len(commits_df)})
- è©•ä¼°ï¼š{'âœ… æ¨¡çµ„åŒ–æ€ç¶­å¥½' if small_ratio >= 0.6 else 'âš ï¸ å»ºè­°æ‹†åˆ†å¤§å‹è®Šæ›´'}

#### C. ä¿®å¾©æ€§æäº¤æ¯”ä¾‹
- ä¿®å¾©ç‡ï¼š**{fix_ratio*100:.1f}%** ({fix_count}/{len(commits_df)})
- è©•ä¼°ï¼š{'âœ… ç¨‹å¼ç¢¼å“è³ªé«˜' if fix_ratio < 0.15 else 'âš ï¸ å»ºè­°åŠ å¼·æ¸¬è©¦'}"""
    
    def _generate_tech_breadth_details(self) -> str:
        """ç”¢ç”ŸæŠ€è¡“å»£åº¦è©³ç´°èªªæ˜"""
        if self.data['code_changes'].empty:
            return "âš ï¸ ç„¡ç¨‹å¼ç¢¼è®Šæ›´è³‡æ–™"
        
        changes_df = self.data['code_changes']
        file_extensions = changes_df['file_path'].apply(
            lambda x: os.path.splitext(str(x))[1].lower() if pd.notna(x) else ''
        )
        
        ignore_extensions = {'', '.md', '.txt', '.json', '.yml', '.yaml', '.xml'}
        file_extensions = file_extensions[~file_extensions.isin(ignore_extensions)]
        
        extension_counts = file_extensions.value_counts().head(10)
        unique_count = file_extensions.nunique()
        
        details = f"- æ¶‰åŠæª”æ¡ˆé¡å‹ï¼š**{unique_count}** ç¨®\n\n"
        details += "**ä¸»è¦æŠ€è¡“æ£§ï¼š**\n"
        for ext, count in extension_counts.items():
            details += f"  - `{ext}`: {count} å€‹æª”æ¡ˆ\n"
        
        return details
    
    def _generate_collaboration_details(self) -> str:
        """ç”¢ç”Ÿå”ä½œèƒ½åŠ›è©³ç´°èªªæ˜"""
        if self.data['commits'].empty:
            return "âš ï¸ ç„¡ Commit è³‡æ–™"
        
        commits_df = self.data['commits']
        total = len(commits_df)
        
        merge_count = commits_df['title'].str.contains(
            r'merge', case=False, regex=True, na=False
        ).sum()
        merge_ratio = merge_count / total
        
        revert_count = commits_df['title'].str.contains(
            r'revert', case=False, regex=True, na=False
        ).sum()
        revert_ratio = revert_count / total
        
        return f"""- Merge Commitsï¼š**{merge_count}** ({merge_ratio*100:.1f}%)
- Revert ç‡ï¼š**{revert_ratio*100:.1f}%**
- è©•ä¼°ï¼š{'âœ… è‰¯å¥½çš„å”ä½œåƒèˆ‡' if merge_ratio > 0.05 and revert_ratio < 0.02 else 'å»ºè­°å¢åŠ åˆ†æ”¯å”ä½œ'}"""
    
    def _generate_code_review_details(self) -> str:
        """ç”¢ç”Ÿ Code Review å“è³ªè©³ç´°èªªæ˜"""
        if self.data['code_reviews'].empty:
            return "âš ï¸ ç„¡ Code Review è³‡æ–™\n\nå»ºè­°ï¼šç©æ¥µåƒèˆ‡ Code Reviewï¼Œæå‡åœ˜éšŠç¨‹å¼ç¢¼å“è³ª"
        
        reviews_df = self.data['code_reviews']
        total_reviews = len(reviews_df)
        
        return f"""- Review åƒèˆ‡æ¬¡æ•¸ï¼š**{total_reviews}**
- è©•ä¼°ï¼š{'âœ… ç©æ¥µåƒèˆ‡' if total_reviews >= 20 else 'âš ï¸ å»ºè­°å¢åŠ  Review åƒèˆ‡åº¦'}"""
    
    def _generate_work_pattern_details(self) -> str:
        """ç”¢ç”Ÿå·¥ä½œæ¨¡å¼è©³ç´°èªªæ˜"""
        if self.data['user_events'].empty:
            return "âš ï¸ ç„¡æ´»å‹•è³‡æ–™"
        
        try:
            events_df = self.data['user_events'].copy()
            events_df['created_at'] = pd.to_datetime(events_df['created_at'])
            events_df['hour'] = events_df['created_at'].dt.hour
            events_df['weekday'] = events_df['created_at'].dt.weekday
            
            work_hours = events_df['hour'].between(9, 18).sum()
            work_hours_ratio = work_hours / len(events_df)
            
            work_days = events_df['weekday'].between(0, 4).sum()
            work_days_ratio = work_days / len(events_df)
            
            return f"""- å·¥ä½œæ™‚æ®µæ´»å‹•ï¼š**{work_hours_ratio*100:.1f}%**
- å·¥ä½œæ—¥æ´»å‹•ï¼š**{work_days_ratio*100:.1f}%**
- è©•ä¼°ï¼š{'âœ… è¦å¾‹çš„å·¥ä½œæ¨¡å¼' if work_hours_ratio >= 0.6 and work_days_ratio >= 0.7 else 'âš ï¸ å»ºè­°èª¿æ•´å·¥ä½œæ™‚é–“åˆ†é…'}"""
        except:
            return "âš ï¸ ç„¡æ³•è§£ææ™‚é–“è³‡æ–™"
    
    def _generate_progress_trend_details(self) -> str:
        """ç”¢ç”Ÿé€²æ­¥è¶¨å‹¢è©³ç´°èªªæ˜"""
        return "- åŸºæ–¼æ™‚é–“åºåˆ—åˆ†æé–‹ç™¼è€…çš„æˆé•·è¶¨å‹¢\n- æ¯”è¼ƒå‰å¾ŒæœŸçš„ Commit å“è³ªè®ŠåŒ–"
    
    def _generate_improvement_suggestions(self, total_score: float) -> str:
        """ç”¢ç”Ÿæ”¹é€²å»ºè­°"""
        suggestions = []
        
        # æ ¹æ“šå„ç¶­åº¦è©•åˆ†æä¾›å»ºè­°
        if self.scores['commit_quality'] < 7.0:
            suggestions.append("ğŸ¯ **æå‡ Commit å“è³ª**ï¼šæ¡ç”¨ Conventional Commits æ ¼å¼ï¼Œæ‹†åˆ†å¤§å‹è®Šæ›´")
        
        if self.scores['tech_breadth'] < 6.0:
            suggestions.append("ğŸ¯ **æ“´å±•æŠ€è¡“å»£åº¦**ï¼šå˜—è©¦å­¸ç¿’æ–°çš„æŠ€è¡“æ£§ï¼Œåƒèˆ‡ä¸åŒé¡å‹çš„å°ˆæ¡ˆ")
        
        if self.scores['code_review'] < 7.0:
            suggestions.append("ğŸ¯ **åŠ å¼· Code Review åƒèˆ‡**ï¼šç©æ¥µå¯©æŸ¥ä»–äººç¨‹å¼ç¢¼ï¼Œæå‡åœ˜éšŠæ•´é«”å“è³ª")
        
        if self.scores['collaboration'] < 6.0:
            suggestions.append("ğŸ¯ **å¢å¼·å”ä½œèƒ½åŠ›**ï¼šå¤šä½¿ç”¨åˆ†æ”¯é–‹ç™¼ï¼Œæ¸›å°‘ç›´æ¥æäº¤åˆ°ä¸»åˆ†æ”¯")
        
        if not suggestions:
            suggestions.append("âœ… **ä¿æŒå„ªç§€è¡¨ç¾**ï¼šç¹¼çºŒä¿æŒé«˜å“è³ªçš„ç¨‹å¼ç¢¼è²¢ç»")
        
        return "\n".join(suggestions)


# ==================== æ–¹æ¡ˆ A: AI æ¨¡å‹åˆ†æå™¨ ====================

class AIModelAnalyzer(IUserAnalyzer):
    """åŸºæ–¼ GitHub Models API çš„ AI åˆ†æ"""
    
    def __init__(self, progress_reporter: Optional[IProgressReporter] = None):
        self.progress = progress_reporter or SilentProgressReporter()
        self.api_key = config.GITHUB_MODELS_API_KEY
        self.api_url = config.GITHUB_MODELS_API_URL
        self.model = config.GITHUB_MODELS_MODEL
    
    def analyze(self, user_data_dir: Path, spec_file: Optional[Path] = None) -> str:
        """åŸ·è¡Œ AI åˆ†æ"""
        self.progress.report_start(f"æ­£åœ¨ä½¿ç”¨ AI åˆ†æ {user_data_dir.name}...")
        
        # æª¢æŸ¥ API Key
        if not self.api_key:
            return self._generate_error_report(
                user_data_dir.name,
                "âŒ éŒ¯èª¤ï¼šæœªè¨­å®š GITHUB_MODELS_API_KEY\n\nè«‹åœ¨ config.py ä¸­è¨­å®šæ‚¨çš„ GitHub Models API Key"
            )
        
        # è¼‰å…¥è³‡æ–™
        data_loader = UserDataLoader(user_data_dir)
        data = data_loader.load_all()
        
        if data.get('commits', pd.DataFrame()).empty:
            return self._generate_error_report(
                user_data_dir.name,
                "âš ï¸ éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° commits.csv æˆ–è³‡æ–™ç‚ºç©º"
            )
        
        # è®€å– spec æª”æ¡ˆ
        spec_content = self._load_spec_file(spec_file)
        
        # çµ„è£ prompt
        prompt = self._build_prompt(data_loader.get_username(), data, spec_content)
        
        # èª¿ç”¨ API
        try:
            report = self._call_api(prompt)
            self.progress.report_complete("AI åˆ†æå®Œæˆ")
            return report
        except Exception as e:
            return self._generate_error_report(
                user_data_dir.name,
                f"âŒ API èª¿ç”¨å¤±æ•—ï¼š{str(e)}"
            )
    
    def _load_spec_file(self, spec_file: Optional[Path]) -> str:
        """è¼‰å…¥åˆ†æè¦æ ¼æª”æ¡ˆ"""
        if spec_file and spec_file.exists():
            with open(spec_file, 'r', encoding='utf-8') as f:
                return f.read()
        
        # å°‹æ‰¾é è¨­ spec æª”æ¡ˆ
        default_paths = [
            Path(__file__).parent.parent / '.copilot/skills/developer-assessment/references/code-quality-analysis-spec.md',
            Path('code-quality-analysis-spec.md'),
            Path('../.copilot/skills/developer-assessment/references/code-quality-analysis-spec.md')
        ]
        
        for path in default_paths:
            if path.exists():
                with open(path, 'r', encoding='utf-8') as f:
                    return f.read()
        
        return "è«‹æ ¹æ“šé–‹ç™¼è€…çš„ Git ç‰ˆæ§è³‡æ–™ï¼Œè©•ä¼°å…¶æŠ€è¡“æ°´å¹³ã€‚"
    
    def _build_prompt(self, username: str, data: Dict[str, pd.DataFrame], spec_content: str) -> str:
        """çµ„è£ AI prompt"""
        # æº–å‚™ CSV è³‡æ–™æ‘˜è¦
        csv_summary = self._summarize_csv_data(data)
        
        prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹è©•åˆ†æ¨™æº–å’Œé–‹ç™¼è€…è³‡æ–™ï¼Œåˆ†æé–‹ç™¼è€… {username} çš„æŠ€è¡“æ°´å¹³ã€‚

# è©•åˆ†æ¨™æº–

{spec_content}

---

# é–‹ç™¼è€…è³‡æ–™

{csv_summary}

---

# è¦æ±‚

è«‹ç”¢ç”Ÿä¸€ä»½å®Œæ•´çš„ Markdown æ ¼å¼åˆ†æå ±å‘Šï¼ŒåŒ…å«ï¼š

1. **ç¸½é«”è©•ä¼°**ï¼šç¸½åˆ†ï¼ˆ0-10ï¼‰ã€ç­‰ç´šï¼ˆé«˜ç´š/ä¸­ç´š/åˆç´šå·¥ç¨‹å¸«ï¼‰
2. **å„ç¶­åº¦è©•åˆ†**ï¼š7 å€‹ç¶­åº¦çš„è©³ç´°è©•åˆ†å’Œåˆ†æ
3. **è©³ç´°åˆ†æ**ï¼šæ¯å€‹ç¶­åº¦çš„å…·é«”æ•¸æ“šå’Œè©•ä¼°
4. **æ”¹é€²å»ºè­°**ï¼šé‡å°æ€§çš„å»ºè­°

è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼Œæ ¼å¼æ¸…æ™°å°ˆæ¥­ã€‚
"""
        return prompt
    
    def _summarize_csv_data(self, data: Dict[str, pd.DataFrame]) -> str:
        """æ‘˜è¦ CSV è³‡æ–™"""
        summary = []
        
        # Statistics
        if not data['statistics'].empty:
            stats = data['statistics'].iloc[0]
            summary.append(f"""## çµ±è¨ˆè³‡æ–™
- ç¸½æäº¤æ•¸ï¼š{stats.get('total_commits', 0)}
- ç¸½æ–°å¢è¡Œæ•¸ï¼š{stats.get('total_additions', 0)}
- ç¸½åˆªé™¤è¡Œæ•¸ï¼š{stats.get('total_deletions', 0)}
- å¹³å‡æ¯æ¬¡è®Šæ›´ï¼š{stats.get('avg_changes_per_commit', 0):.2f} è¡Œ
- Merge Requestsï¼š{stats.get('total_merge_requests', 0)}
- Code Reviewsï¼š{stats.get('total_code_reviews', 0)}
""")
        
        # Commits æ¨£æœ¬
        if not data['commits'].empty:
            commits_sample = data['commits'].head(20)
            summary.append(f"""## Commits æ¨£æœ¬ï¼ˆå‰ 20 ç­†ï¼‰
| Commit Message | æ–°å¢ | åˆªé™¤ | ç¸½è¨ˆ |
|----------------|------|------|------|
""")
            for _, row in commits_sample.iterrows():
                title = str(row.get('title', ''))[:50]
                summary.append(f"| {title} | {row.get('additions', 0)} | {row.get('deletions', 0)} | {row.get('total', 0)} |")
        
        # Code Reviews
        if not data['code_reviews'].empty:
            summary.append(f"\n## Code Reviews\n- ç¸½åƒèˆ‡æ¬¡æ•¸ï¼š{len(data['code_reviews'])}")
        
        # File Types
        if not data['code_changes'].empty:
            file_extensions = data['code_changes']['file_path'].apply(
                lambda x: os.path.splitext(str(x))[1].lower() if pd.notna(x) else ''
            )
            extension_counts = file_extensions.value_counts().head(10)
            summary.append("\n## æª”æ¡ˆé¡å‹åˆ†ä½ˆ")
            for ext, count in extension_counts.items():
                if ext:
                    summary.append(f"- {ext}: {count} å€‹æª”æ¡ˆ")
        
        return "\n".join(summary)
    
    def _call_api(self, prompt: str) -> str:
        """èª¿ç”¨ GitHub Models API"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": self.model,
            "messages": [
                {
                    "role": "system",
                    "content": "ä½ æ˜¯ä¸€ä½è³‡æ·±çš„ç¨‹å¼ç¢¼å“è³ªè©•ä¼°å°ˆå®¶ï¼Œç²¾é€šå¾ Git ç‰ˆæ§è³‡æ–™è©•ä¼°é–‹ç™¼è€…æŠ€è¡“æ°´å¹³ã€‚"
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "temperature": 0.7,
            "max_tokens": 4000
        }
        
        response = requests.post(
            self.api_url,
            headers=headers,
            json=payload,
            timeout=60
        )
        
        if response.status_code != 200:
            raise Exception(f"API éŒ¯èª¤ {response.status_code}: {response.text}")
        
        result = response.json()
        
        # æå– AI ç”Ÿæˆçš„å…§å®¹
        if 'choices' in result and len(result['choices']) > 0:
            return result['choices'][0]['message']['content']
        else:
            raise Exception("API å›æ‡‰æ ¼å¼éŒ¯èª¤")
    
    def _generate_error_report(self, username: str, error_message: str) -> str:
        """ç”¢ç”ŸéŒ¯èª¤å ±å‘Š"""
        return f"""# {username} æŠ€è¡“æ°´å¹³åˆ†æå ±å‘Š

**ç”Ÿæˆæ™‚é–“ï¼š** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**åˆ†ææ–¹å¼ï¼š** AI æ¨¡å‹åˆ†æï¼ˆGitHub Models APIï¼‰

---

{error_message}
"""


# ==================== åˆ†ææœå‹™ ====================

class UserAnalysisService:
    """é–‹ç™¼è€…åˆ†ææœå‹™"""
    
    def __init__(
        self,
        analyzer: IUserAnalyzer,
        data_source: Path,
        output_dir: Path,
        progress_reporter: Optional[IProgressReporter] = None
    ):
        self.analyzer = analyzer
        self.data_source = data_source
        self.output_dir = output_dir
        self.progress = progress_reporter or SilentProgressReporter()
        self.analysis_results: List[Dict[str, Any]] = []  # æ”¶é›†åˆ†æçµæœ
    
    def execute(
        self,
        username: Optional[str] = None,
        spec_file: Optional[Path] = None
    ) -> None:
        """åŸ·è¡Œåˆ†æ"""
        # æ‰¾åˆ°è¦åˆ†æçš„ä½¿ç”¨è€…ç›®éŒ„
        user_dirs = self._find_user_directories(username)
        
        if not user_dirs:
            print(f"âš ï¸ æ‰¾ä¸åˆ°ä½¿ç”¨è€…è³‡æ–™ï¼š{username or 'å…¨éƒ¨'}")
            return
        
        total = len(user_dirs)
        self.progress.report_start(f"é–‹å§‹åˆ†æ {total} ä½ä½¿ç”¨è€…...")
        
        # æ¸…ç©ºä¹‹å‰çš„çµæœ
        self.analysis_results = []
        
        for i, user_dir in enumerate(user_dirs, 1):
            print(f"\n{'='*70}")
            print(f"[{i}/{total}] åˆ†æï¼š{user_dir.name}")
            print(f"{'='*70}")
            
            # åŸ·è¡Œåˆ†æ
            report = self.analyzer.analyze(user_dir, spec_file)
            
            # å„²å­˜å ±å‘Š
            output_path = self.output_dir / 'users' / user_dir.name / 'analysis-result.md'
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(report)
            
            print(f"âœ… å ±å‘Šå·²å„²å­˜ï¼š{output_path}")
            
            # æ”¶é›†è©•åˆ†è³‡æ–™ï¼ˆåƒ… CodeBasedAnalyzer æœ‰ scores å±¬æ€§ï¼‰
            if isinstance(self.analyzer, CodeBasedAnalyzer):
                self.analysis_results.append({
                    'username': user_dir.name,
                    'total_score': self.analyzer.total_score,
                    'level': self.analyzer.level,
                    'scores': self.analyzer.scores.copy()
                })
        
        self.progress.report_complete(f"å®Œæˆ {total} ä½ä½¿ç”¨è€…åˆ†æ")
        
        # ç”¢ç”Ÿå½™ç¸½å ±å‘Š
        if len(self.analysis_results) > 0:
            self._generate_summary_report()
    
    def _generate_summary_report(self) -> None:
        """ç”¢ç”Ÿæ‰€æœ‰ä½¿ç”¨è€…çš„å½™ç¸½å ±å‘Š"""
        print(f"\n{'='*70}")
        print("æ­£åœ¨ç”¢ç”Ÿå½™ç¸½å ±å‘Š...")
        print(f"{'='*70}")
        
        # ç”¢ç”Ÿ Markdown è¡¨æ ¼
        report_lines = [
            "# é–‹ç™¼è€…æŠ€è¡“æ°´å¹³åˆ†æå½™ç¸½å ±å‘Š",
            "",
            f"**ç”Ÿæˆæ™‚é–“ï¼š** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  ",
            f"**åˆ†æäººæ•¸ï¼š** {len(self.analysis_results)} ä½é–‹ç™¼è€…  ",
            f"**åˆ†ææ–¹å¼ï¼š** ç¨‹å¼ç¢¼è¨ˆç®—ï¼ˆCode-Based Analysisï¼‰",
            "",
            "---",
            "",
            "## ğŸ“Š æ•´é«”è©•åˆ†ç¸½è¦½",
            "",
            "| username | ç¨‹å¼ç¢¼è²¢ç»é‡ | æŠ€è¡“å»£åº¦ | å”ä½œèƒ½åŠ› | Code Review å“è³ª | å·¥ä½œæ¨¡å¼ | é€²æ­¥è¶¨å‹¢ |",
            "|----------|-------------|---------|---------|-----------------|---------|---------|"
        ]
        
        # æ’åºï¼šæŒ‰ç¸½åˆ†é™åº
        sorted_results = sorted(
            self.analysis_results, 
            key=lambda x: x['total_score'], 
            reverse=True
        )
        
        for result in sorted_results:
            username = result['username']
            scores = result['scores']
            
            # å»ºç«‹è¡¨æ ¼è¡Œï¼ˆåƒ…åŒ…å«éœ€è¦çš„æ¬„ä½ï¼‰
            row = (
                f"| {username} "
                f"| {scores['contribution']:.2f} "
                f"| {scores['tech_breadth']:.2f} "
                f"| {scores['collaboration']:.2f} "
                f"| {scores['code_review']:.2f} "
                f"| {scores['work_pattern']:.2f} "
                f"| {scores['progress_trend']:.2f} |"
            )
            report_lines.append(row)
        
        # æ–°å¢çµ±è¨ˆè³‡è¨Š
        report_lines.extend([
            "",
            "---",
            "",
            "## ğŸ“ˆ çµ±è¨ˆè³‡è¨Š",
            ""
        ])
        
        # è¨ˆç®—å„ç­‰ç´šäººæ•¸
        level_counts = {}
        total_scores = []
        for result in self.analysis_results:
            level = result['level']
            level_counts[level] = level_counts.get(level, 0) + 1
            total_scores.append(result['total_score'])
        
        # ç­‰ç´šåˆ†ä½ˆ
        report_lines.append("### ç­‰ç´šåˆ†ä½ˆ")
        report_lines.append("")
        for level, count in sorted(level_counts.items(), key=lambda x: x[1], reverse=True):
            percentage = count / len(self.analysis_results) * 100
            report_lines.append(f"- **{level}**ï¼š{count} ä½ ({percentage:.1f}%)")
        
        # åˆ†æ•¸çµ±è¨ˆ
        if total_scores:
            avg_score = sum(total_scores) / len(total_scores)
            max_score = max(total_scores)
            min_score = min(total_scores)
            
            report_lines.extend([
                "",
                "### åˆ†æ•¸çµ±è¨ˆ",
                "",
                f"- **å¹³å‡åˆ†ï¼š** {avg_score:.2f}",
                f"- **æœ€é«˜åˆ†ï¼š** {max_score:.2f}",
                f"- **æœ€ä½åˆ†ï¼š** {min_score:.2f}",
            ])
        
        # å„ç¶­åº¦å¹³å‡åˆ†
        dimension_names = {
            'contribution': 'ç¨‹å¼ç¢¼è²¢ç»é‡',
            'commit_quality': 'Commit å“è³ª',
            'tech_breadth': 'æŠ€è¡“å»£åº¦',
            'collaboration': 'å”ä½œèƒ½åŠ›',
            'code_review': 'Code Review å“è³ª',
            'work_pattern': 'å·¥ä½œæ¨¡å¼',
            'progress_trend': 'é€²æ­¥è¶¨å‹¢'
        }
        
        dimension_avgs = {}
        for dim_key in dimension_names.keys():
            scores = [r['scores'][dim_key] for r in self.analysis_results]
            dimension_avgs[dim_key] = sum(scores) / len(scores)
        
        report_lines.extend([
            "",
            "### å„ç¶­åº¦å¹³å‡åˆ†",
            ""
        ])
        
        for dim_key, dim_name in dimension_names.items():
            avg = dimension_avgs[dim_key]
            report_lines.append(f"- **{dim_name}**ï¼š{avg:.2f}")
        
        # æ–°å¢èªªæ˜
        report_lines.extend([
            "",
            "---",
            "",
            "## ğŸ“ è©•åˆ†èªªæ˜",
            "",
            "**ç­‰ç´šæ¨™æº–ï¼š**",
            "- ğŸ† **é«˜ç´šå·¥ç¨‹å¸«** (8-10åˆ†)ï¼šMessage è¦ç¯„ç‡ 90%+ã€å°å‹è®Šæ›´ä½”æ¯” 80%+ã€æ¶‰åŠ 3+ æŠ€è¡“æ£§",
            "- â­ **ä¸­ç´šå·¥ç¨‹å¸«** (5-7åˆ†)ï¼šMessage è¦ç¯„ç‡ 60-90%ã€è®Šæ›´ç²’åº¦åˆç†ã€2-3 ç¨®æŠ€è¡“æ£§",
            "- ğŸŒ± **åˆç´šå·¥ç¨‹å¸«** (1-4åˆ†)ï¼šMessage ä¸è¦ç¯„ã€å¤§é‡ä¿®å¾©æ€§æäº¤ã€å–®ä¸€æŠ€è¡“æ£§",
            "",
            "**ç¶­åº¦æ¬Šé‡ï¼š**",
            "- Commit å“è³ªï¼š23% â­ æœ€é‡è¦",
            "- æŠ€è¡“å»£åº¦ï¼š18%",
            "- é€²æ­¥è¶¨å‹¢ï¼š15%",
            "- ç¨‹å¼ç¢¼è²¢ç»é‡ï¼š12%",
            "- å”ä½œèƒ½åŠ›ï¼š12%",
            "- Code Review å“è³ªï¼š10%",
            "- å·¥ä½œæ¨¡å¼ï¼š10%",
            "",
            "---",
            "",
            "**åˆ†æå·¥å…·ç‰ˆæœ¬ï¼š** v1.0  ",
            "**è©•åˆ†æ¨™æº–ï¼š** åŸºæ–¼ code-quality-analysis-spec.md"
        ])
        
        # å„²å­˜å½™ç¸½å ±å‘Š
        summary_path = self.output_dir / 'users' / 'all-user-analysis-result.md'
        summary_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        
        print(f"âœ… å½™ç¸½å ±å‘Šå·²å„²å­˜ï¼š{summary_path}")
        print(f"   å…±åˆ†æ {len(self.analysis_results)} ä½é–‹ç™¼è€…")
    
    def _find_user_directories(self, username: Optional[str]) -> List[Path]:
        """å°‹æ‰¾ä½¿ç”¨è€…è³‡æ–™ç›®éŒ„"""
        if not self.data_source.exists():
            return []
        
        if username:
            # æŒ‡å®šä½¿ç”¨è€…
            user_dir = self.data_source / username
            if user_dir.exists() and user_dir.is_dir():
                return [user_dir]
            return []
        else:
            # å…¨éƒ¨ä½¿ç”¨è€…
            return [d for d in self.data_source.iterdir() if d.is_dir()]
